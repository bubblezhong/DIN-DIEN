{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import numpy as np\n",
    "from data_iterator import DataIterator,prepare_data\n",
    "import tensorflow as tf\n",
    "# from model import *\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "# from utils import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "# import tensorflow.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 18\n",
    "HIDDEN_SIZE = 18 * 2\n",
    "ATTENTION_SIZE = 18 * 2\n",
    "best_auc = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./data/local_train_splitByUser\"\n",
    "test_file = \"./data/local_test_splitByUser\"\n",
    "uid_voc = \"./data/uid_voc.pkl\"\n",
    "mid_voc = \"./data/mid_voc.pkl\"\n",
    "cat_voc = \"./data/cat_voc.pkl\"\n",
    "batch_size = 128\n",
    "maxlen = 100\n",
    "test_iter = 100\n",
    "save_iter = 100\n",
    "model_type = 'DNN'\n",
    "seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"dnn_save_path/ckpt_noshuff\" + model_type + str(seed)\n",
    "best_model_path = \"dnn_best_model/ckpt_noshuff\" + model_type + str(seed)\n",
    "#     gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "train_data = DataIterator(train_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen, shuffle_each_epoch=False)\n",
    "test_data = DataIterator(test_file, uid_voc, mid_voc, cat_voc, batch_size, maxlen)\n",
    "n_uid, n_mid, n_cat = train_data.get_n()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src, tgt in train_data:\n",
    "    uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = prepare_data(src, tgt, maxlen, return_neg=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 100), (128, 100), (128, 100, 5))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_mask.shape,noclk_mids[:,:,1].shape, noclk_cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dice_layer(layers.Layer):\n",
    "    def __init__(self,axis=-1, epsilon = 1e-10):\n",
    "#         super(dice_layer, self).__init__()\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        rand = tf.random_normal_initializer()(shape=[input_shape[-1]])\n",
    "        self.alpha = tf.Variable(rand, dtype=tf.float32, name=\"alpha\")\n",
    "    def call(self, x):\n",
    "        input_shape = list(x.shape)\n",
    "        \n",
    "        # 需要进行reduce计算的轴\n",
    "        reducetion_axes = list(range(len(input_shape)))  # [0,1]\n",
    "        del reducetion_axes[self.axis]      # [0]\n",
    "        \n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]  # [1,72]\n",
    "        # 计算每个样本的某个特征f的均值        \n",
    "        mean = tf.reduce_mean(x, axis= reducetion_axes) #(f,) 一共f个特征\n",
    "        brodcast_mean = tf.reshape(mean, broadcast_shape) # reshape (1,f)\n",
    "        \n",
    "        # 计算方差\n",
    "        std = tf.reduce_mean(tf.square(x-brodcast_mean) + self.epsilon, axis=reducetion_axes)\n",
    "        std = tf.sqrt(std)\n",
    "        \n",
    "        # 还原\n",
    "        brodcast_std = tf.reshape(std,broadcast_shape)\n",
    "        \n",
    "        # 标准化，_x的shape不变\n",
    "        x_normal = (x-brodcast_mean)/ (brodcast_std + self.epsilon)\n",
    "        \n",
    "        # 以上过程即为BatchNormalization：\n",
    "        # x_normed = layer.BatchNormalization(center=False, scale=False)(_x)\n",
    "        \n",
    "        # 标准化后使用 sigmoid 函数得到 x_p\n",
    "        x_p = tf.sigmoid(x_normal)\n",
    "        return self.alpha * (1.0 - x_p) * x + x_p * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, fc_config=[128,64,1]):\n",
    "        super(fc_layer, self).__init__()\n",
    "        self.fc_layers = []\n",
    "        self.dice_layers = []      \n",
    "        for s in fc_config[:-1]:\n",
    "            self.fc_layers.append(layers.Dense(s, name='dense'+str(s)))\n",
    "            \n",
    "        for s in fc_config[:-1]:\n",
    "            self.dice_layers.append(dice_layer())\n",
    "\n",
    "        self.fc_layers.append(layers.Dense( fc_config[-1]))\n",
    "        \n",
    "    def call(self, inp):\n",
    "        fc_out = inp\n",
    "        for i, _ in enumerate(self.fc_layers[:-1]):\n",
    "                fc_out = self.dice_layers[i](self.fc_layers[i](fc_out))\n",
    "        fc_out =  self.fc_layers[-1](fc_out)       \n",
    "        return fc_out\n",
    "#         self.y_hat = tf.nn.softmax(dnn3) + 0.00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class din_attention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(din_attention, self).__init__()\n",
    "        self.fc_part = fc_layer([80,40,1])\n",
    "        \n",
    "    def call(self, item_emb, hist_item_emb,item_mask,softmax_stag=True, mode='SUM'):\n",
    "        \"\"\"\n",
    "        item_emb: (B,D)\n",
    "        hist_item_emb: (B,T,D)\n",
    "        item_mask: (B,T)\n",
    "        此处的D均表示cat_embedding维度+item_embedding维度\n",
    "        \"\"\"\n",
    "        masks = tf.math.equal(item_mask, tf.ones_like(item_mask)) # (B,T)\n",
    "        item_size = item_emb.shape[-1]  \n",
    "        item_hist_size = hist_item_emb.shape[-1] \n",
    "        # 为了方便计算相似度，将item复制至个数与hist_item长度相同\n",
    "        queries = tf.tile(item_emb, [1,hist_item_emb.shape[1]]) #(B, D *T)\n",
    "        queries = tf.reshape(queries, [-1,hist_item_emb.shape[1],hist_item_emb.shape[2]])  # (B,T,D)\n",
    "        din_all = tf.concat([queries, hist_item_emb,queries-hist_item_emb, queries*hist_item_emb],axis=-1)  #(B,T,D*4)\n",
    "        # 相似度得分\n",
    "        similar_scores = self.fc_part(din_all) # (B,T,1)\n",
    "        similar_scores = tf.reshape(similar_scores, [-1,1,hist_item_emb.shape[1]])  # (B,1,T)\n",
    "#         print(din_all.shape,'scores',similar_scores.shape)\n",
    "        \n",
    "        # tf.where 根据mask将得分与hist_item_emb对应的位置匹配填充，不存在的地方，填充为极大值。\n",
    "        key_masks = tf.expand_dims(masks, 1) # (B, 1, T)\n",
    "        # 计分器容器\n",
    "        paddings = tf.ones_like(similar_scores) * (-2 ** 32 + 1)\n",
    "        \n",
    "        scores = tf.where(key_masks, similar_scores, paddings) # (B, 1, T)\n",
    "        \n",
    "        if softmax_stag:\n",
    "            scores = tf.nn.softmax(scores) # (B,1,T)\n",
    "        scores = tf.reshape(scores,[-1,hist_item_emb.shape[1],1])    \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class auxiliary_net(tf.keras.layers.Layer):\n",
    "    def __init__(self, auxi_config = [100,50,2] ):\n",
    "        super().__init__() \n",
    "        self.bh_nor = layers.BatchNormalization()\n",
    "        self.auxi_layers = []\n",
    "        for n in auxi_config:\n",
    "            self.auxi_layers.append(layers.Dense(n,activation='sigmoid',name='auxiliary_net'+str(n)))\n",
    "    def call(self,inp):\n",
    "#         print('inp',inp)\n",
    "        dnn_res = self.bh_nor(inp, training=True)\n",
    "#         dnn_res=inp\n",
    "        for i in range(len(self.auxi_layers)):\n",
    "            dnn_res = self.auxi_layers[i](dnn_res)\n",
    "        return dnn_res + 0.00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class auxi_loss_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dnn_part = auxiliary_net([100,50,2])\n",
    "    def call(self, rnn_outputs,hist_item_emb,noclk_his_emb,item_mask):\n",
    "        \n",
    "#         print(rnn_outputs.shape,hist_item_emb.shape, noclk_his_emb.shape,item_mask.shape)\n",
    "        # (128, 99, 64) (128, 99, 16) (128, 99, 16) (128, 99)  (B, SL-1, HD) \n",
    "        click_input = tf.concat([rnn_outputs, hist_item_emb],axis=-1)  # (128,99,80)\n",
    "        noclick_input = tf.concat([rnn_outputs, noclk_his_emb], axis=-1)\n",
    "         # 取出预测结果中点击的概率\n",
    "        click_prop = self.dnn_part(click_input)[:,:,0] \n",
    "         # 取出预测结果中不点击的概率\n",
    "        noclick_prop = self.dnn_part(noclick_input)[:,:,0] #  (128, 99, 2)-> (128, 99)\n",
    "        # 计算loss\n",
    "        click_loss = - tf.math.log(click_prop) * item_mask\n",
    "        noclick_loss = -tf.math.log(1.0-noclick_prop) * item_mask\n",
    "        loss_ = tf.reduce_mean(click_loss + noclick_loss)\n",
    "        return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUGRUCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # RNN单元的必须参数，代表每个时间步的输出维度\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "    def build(self, input_shape):\n",
    "        # t时刻 输入的 x_t的维度\n",
    "        dim_xt = input_shape[0][-1]\n",
    "        # 重置门对t时刻输入数据的权重\n",
    "        self.W_R_x = tf.Variable(tf.random.normal(shape=[dim_xt, self.units]), name='W_R_x')\n",
    "        # 重置门对t时刻的隐藏状态state的权重参数\n",
    "        self.W_R_s = tf.Variable(tf.random.normal(shape=[self.units, self.units], name='W_R_s'))\n",
    "        # 重置门的偏置\n",
    "        self.W_R_b = tf.Variable(tf.random.normal(shape=[self.units]), name='W_R_b')\n",
    "        \n",
    "        # 更新门的对输入数据的权重\n",
    "        self.W_U_x = tf.Variable(tf.random.normal(shape=[dim_xt, self.units]), name='W_U_x')\n",
    "        # 更新门对t时刻的隐藏状态state的权重参数\n",
    "        self.W_U_s = tf.Variable(tf.random.normal(shape=[self.units, self.units], name='W_U_s'))\n",
    "        # 更新门的偏置\n",
    "        self.W_U_b = tf.Variable(tf.random.normal(shape=[self.units]), name='W_U_b')\n",
    "        \n",
    "        # 计算当前时刻单元的状态: hat_h_t\n",
    "        # 输入数据的权重\n",
    "        self.W_H_x = tf.Variable(tf.random.normal(shape=[dim_xt, self.units]), name='W_H_x')\n",
    "        # 上一个单元隐藏层的权重\n",
    "        self.W_H_s = tf.Variable(tf.random.normal(shape=[self.units, self.units]), name='W_H_s')\n",
    "        # 偏置\n",
    "        self.W_H_b = tf.Variable(tf.random.normal(shape=[self.units]), name='W_H_b')  \n",
    "    def call(self, inputs, states):\n",
    "        x_t, att_score = inputs\n",
    "        states = states[0]\n",
    "        \n",
    "        \"\"\"\n",
    "        x_t: shape=(B, 2D)\n",
    "        states: hidden_state(t-1), shape=(B, units)\n",
    "        att_score: attention_score(t), shape=(B,1)\n",
    "        \"\"\"\n",
    "        # 重置门\n",
    "        r_t = tf.sigmoid( tf.matmul(x_t,self.W_R_x) +  tf.matmul(states, self.W_R_s) + self.W_R_b ) # (B, units,)\n",
    "        # 更新门\n",
    "        u_t = tf.sigmoid(tf.matmul(x_t, self.W_U_x) + tf.matmul(states, self.W_U_s) + self.W_U_b) # (B, units,)\n",
    "        # 带有注意力机制的更新门\n",
    "        a_u_t = tf.multiply(att_score, u_t) # (B,1) * (B, units) = (B, units)\n",
    "        # 当前时刻单元的状态: hat_h_t (B,units)\n",
    "        hat_h_t = tf.tanh(tf.matmul(x_t, self.W_H_x)\n",
    "                          + tf.matmul(tf.multiply(r_t, states) , self.W_H_s)\n",
    "                          + self.W_H_b\n",
    "                          )\n",
    "        # 输出值\n",
    "        h_t = tf.multiply(1-a_u_t, states) + tf.multiply(a_u_t, hat_h_t)\n",
    "        # 对gru而言，当前时刻的output与传递给下一时刻的state相同\n",
    "        next_state = h_t\n",
    "        \n",
    "        return h_t, next_state # 第一个表示output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DIEN(tf.keras.Model):\n",
    "    def __init__(self, user_count, item_count, cate_count, user_dim, item_dim, cate_dim,EMBEDDING_DIM, HIDDEN_SIZE, ATTENTION_SIZE, use_negsampling = False):\n",
    "        super(DIEN,self).__init__()\n",
    "        # 初始化用户、商品、类别 Embedding. l2正则化\n",
    "        self.user_emb= layers.Embedding(user_count,user_dim,\n",
    "                                              embeddings_regularizer=tf.keras.regularizers.l2(0.1),name='uid_embedding')\n",
    "        self.item_emb = layers.Embedding(item_count,item_dim,\n",
    "                                              embeddings_regularizer=tf.keras.regularizers.l2(0.1),name='item_embedding')\n",
    "        self.cate_emb = layers.Embedding(cate_count,cate_dim,\n",
    "                                              embeddings_regularizer=tf.keras.regularizers.l2(0.1),name='cat_embedding')\n",
    "         # 构造模型的全连接层\n",
    "        self.fc = fc_layer([128,60,2])\n",
    "        self.gru = layers.GRU(64, return_sequences=True)\n",
    "        self.auxi_loss = auxi_loss_layer()\n",
    "        self.din_att = din_attention()\n",
    "        \n",
    "        self.AUGRU = layers.RNN(AUGRUCell(16)) # 初始化传入隐藏层的维度\n",
    "  \n",
    "    def get_emb(self,usr,item,cat,item_his, cat_his,noclk_mids, noclk_cats):\n",
    "        user_emb = self.user_emb(usr)\n",
    "        item_emb = self.item_emb(item)\n",
    "        cat_emb = self.cate_emb(cat)\n",
    "        item_his_emb = self.item_emb(item_his)\n",
    "        cat_his_emb = self.item_emb(cat_his) \n",
    "        \n",
    "        noclk_mids = self.item_emb(noclk_mids[:,:,1])\n",
    "        noclk_cats = self.item_emb(noclk_cats[:,:,1])\n",
    "        # 合并 concat\n",
    "        new_item_emb = tf.concat([item_emb,cat_emb], axis=-1)\n",
    "        new_item_his_emb = tf.concat([item_his_emb, cat_his_emb], axis=-1) \n",
    "        new_noclk_his_emb = tf.concat([noclk_mids, noclk_mids], axis=-1) \n",
    "        return user_emb,new_item_emb,new_item_his_emb,new_noclk_his_emb\n",
    "    \n",
    "    def call(self,usr,item,cat,item_his, cat_his,item_mask,sl,noclk_mids, noclk_cats):\n",
    "        user_emb, item_emb, hist_item_emb,noclk_his_emb = self.get_emb(usr, item,cat, item_his,cat_his,noclk_mids, noclk_cats)\n",
    "#         hist_item_emb_sum = tf.reduce_sum(hist_item_emb,axis=1)\n",
    "        bool_mask = tf.cast(item_mask, tf.bool)\n",
    "        gru_res = self.gru(hist_item_emb,mask=bool_mask)  #(B,SL,HD) 其中，SL表示sequence len, HD表示hidden Denmension\n",
    "        auxi_loss = self.auxi_loss(gru_res[:,:-1,:],hist_item_emb[:,1:,:],noclk_his_emb[:,1:,:],item_mask[:,1:])\n",
    "        attention_scores = self.din_att(item_emb, hist_item_emb,item_mask)  # (B, SL)\n",
    "#         print(gru_res.shape,auxi_loss,attention_scores)\n",
    "        \n",
    "        augru_output = self.AUGRU((gru_res, attention_scores),mask=bool_mask) # (B,16)\n",
    "        inp = tf.concat( [user_emb, item_emb,augru_output], axis=-1)\n",
    "        out = self.fc(inp)\n",
    "        y_hat = tf.nn.softmax(out) +  0.00000001\n",
    "        return y_hat,auxi_loss\n",
    "    \n",
    "# (uids, mids, cats, mid_his, cat_his, mid_mask, sl,noclk_mids, noclk_cats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f725a42fd10>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/ddbb/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 4198, in <genexpr>\n",
      "    for ta, out in zip(output_ta_t, flat_new_output))  File \"/home/ddbb/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "batch 10: loss 2.005356\n",
      "batch 20: loss 1.854979\n",
      "batch 30: loss 1.990494\n",
      "batch 40: loss 1.879137\n",
      "batch 50: loss 1.950802\n"
     ]
    }
   ],
   "source": [
    "din_model = DIEN(n_uid, n_mid, n_cat,8,8,8,EMBEDDING_DIM, HIDDEN_SIZE,ATTENTION_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "i = 0  \n",
    "for src, tgt in train_data:\n",
    "    i += 1\n",
    "    uids, mids, cats, mid_his, cat_his, mid_mask, target, sl, noclk_mids, noclk_cats = prepare_data(src, tgt, maxlen, return_neg=True)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred,auxi_loss = din_model(uids, mids, cats, mid_his, cat_his, mid_mask, sl,noclk_mids, noclk_cats)\n",
    "        # loss = -tf.math.reduce_mean(tf.math.log(y_pred)*target)\n",
    "        \n",
    "        loss = tf.keras.losses.categorical_crossentropy(target, y_pred)\n",
    "        loss = tf.reduce_mean(loss) + auxi_loss\n",
    "        # print('batch %d: loss %f'%(i, loss.numpy()))\n",
    "        if i % 10 == 0:\n",
    "            print('batch %d: loss %f'%(i, loss.numpy()))\n",
    "            if i== 50: break\n",
    "    grads = tape.gradient(loss, din_model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, din_model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast([0,1], tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
